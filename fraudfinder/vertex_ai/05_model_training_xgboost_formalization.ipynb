{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraudfinder - Model training and deployment using Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/05_model_training_xgboost_formalization.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/05_model_training_xgboost_formalization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/fraudfinder/blob/main/vertex_ai/05_model_training_xgboost_formalization.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration settings from the setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the `00_environment_setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Fraudfinder](https://github.com/googlecloudplatform/fraudfinder) is a series of labs on how to build a real-time fraud detection system on Google Cloud. Throughout the Fraudfinder labs, you will learn how to read historical bank transaction data stored in data warehouse, read from a live stream of new transactions, perform exploratory data analysis (EDA), do feature engineering, ingest features into a feature store, train a model using feature store, register your model in a model registry, evaluate your model, deploy your model to an endpoint, do real-time inference on your model with feature store, and monitor your model.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn how to:\n",
    "\n",
    "* Build a Vertex AI dataset\n",
    "* Build a Docker container and train a custom XGBoost model using Vertex AI\n",
    "* Evaluate the model locally\n",
    "* Deploy the model to Vertex AI as an endpoint. \n",
    "\n",
    "This tutorial uses the following Google Cloud data analytics and services:\n",
    "\n",
    "- [BigQuery](https://cloud.google.com/bigquery/)\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/)\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* BigQuery\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [BigQuery Pricing](https://cloud.google.com/bigquery/pricing), [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Load old config, and create a new one. This need to be done in Notebook 1. To move after test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env_v02.py\n",
    "#print(config.n)\n",
    "exec(config.n)\n",
    "\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "UPLOAD_BUCKET = f\"model-upload-{PROJECT_ID}\"\n",
    "AGENT_BUCKET = f\"ai-workshops-{ID}\"\n",
    "TRAINING_DS_SIZE = 1000\n",
    "\n",
    "# Replace Region here\n",
    "#ID = {ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mPROJECT_ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mBUCKET_NAME: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mREGION: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREGION\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;124mID: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mID\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mCUSTOMER_ENTITY_ID: customer\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mCUSTOMER_ENTITY_ID_FIELD: customer_id\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mTERMINAL_ENTITY_ID: terminal\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mTERMINALS_ENTITY_ID_FIELD: terminal_id\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mFEATURESTORE_ID: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraudfinder_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mFEATUREVIEW_ID: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraudfinder_view_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mNETWORK: fraud-finder-network\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mSUBNET: https://www.googleapis.com/compute/v1/projects/fraud-finder-lab/regions/us-central1/subnetworks/us-central1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mMODEL_REGISTRY: ff_model\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mRAW_BQ_TRANSACTION_TABLE_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tx.tx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mRAW_BQ_LABELS_TABLE_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tx.txlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mFEATURES_BQ_TABLE_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tx.wide_features_table\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mFEATURE_TIME: feature_ts\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mONLINE_STORAGE_NODES: 1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mSUBSCRIPTION_NAME: ff-tx-for-feat-eng-sub\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mSUBSCRIPTION_PATH: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/subscriptions/ff-tx-for-feat-eng-sub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mDROP_COLUMNS:\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m- timestamp\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m- entity_type_customer\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m- entity_type_terminal\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mFEAT_COLUMNS:\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_14day_window\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_15min_window\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_1day_window\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_30min_window\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_60min_window\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m- customer_id_avg_amount_7day_window\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_14day_window\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_15min_window\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_1day_window\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_30min_window\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_60min_window\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m- customer_id_nb_tx_7day_window\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m- terminal_id_avg_amount_15min_window\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m- terminal_id_avg_amount_30min_window\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m- terminal_id_avg_amount_60min_window\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_14day_window\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_15min_window\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_1day_window\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_30min_window\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_60min_window\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m- terminal_id_nb_tx_7day_window\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m- terminal_id_risk_14day_window\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m- terminal_id_risk_1day_window\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124m- terminal_id_risk_7day_window\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m- tx_amount\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124mTARGET_COLUMN: tx_fraud\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124mDATA_SCHEMA:\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m  timestamp: object\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m  tx_amount: float64\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m  tx_fraud: Int64\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m  entity_type_customer: Int64\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_1day_window: Int64\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_7day_window: Int64\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_14day_window: Int64\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_1day_window: float64\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_7day_window: float64\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_14day_window: float64\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_15min_window: Int64\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_15min_window: float64\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_30min_window: Int64\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_30min_window: float64\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124m  customer_id_nb_tx_60min_window: Int64\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124m  customer_id_avg_amount_60min_window: float64\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m  entity_type_terminal: Int64\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_1day_window: Int64\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_7day_window: Int64\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_14day_window: Int64\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m  terminal_id_risk_1day_window: float64\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124m  terminal_id_risk_7day_window: float64\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m  terminal_id_risk_14day_window: float64\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_15min_window: Int64\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124m  terminal_id_avg_amount_15min_window: float64\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_30min_window: Int64\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124m  terminal_id_avg_amount_30min_window: float64\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m  terminal_id_nb_tx_60min_window: Int64\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124m  terminal_id_avg_amount_60min_window: float64\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124mMODEL_NAME: ff_model\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124mEXPERIMENT_NAME: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mff-experiment-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124mDATA_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fraudfinder/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124mTRAIN_DATA_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fraudfinder/data/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124mREAD_INSTANCES_TABLE: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124mREAD_INSTANCES_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbq://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tx.ground_truth_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124mDATASET_NAME: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_finder_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124mJOB_NAME: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraudfinder-train-xgb-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124mENDPOINT_NAME: ff_model_endpoint\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124mMODEL_SERVING_IMAGE_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124mIMAGE_REPOSITORY: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraudfinder-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124mIMAGE_NAME: dask-xgb-classificator\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124mIMAGE_TAG: latest\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124mIMAGE_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-central1-docker.pkg.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fraudfinder-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/dask-xgb-classificator:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124mTRAIN_COMPUTE: e2-standard-4\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124mDEPLOY_COMPUTE: n1-standard-4\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124mBASE_IMAGE: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython:3.10\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124mPIPELINE_NAME: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud-finder-xgb-pipeline-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124mPIPELINE_ROOT: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fraudfinder/pipelines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124mBQ_DATASET: tx\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124mMETRICS_URI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fraudfinder/deliverables/metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124mAVG_PR_THRESHOLD: 0.2\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124mMODEL_THRESHOLD: 0.5\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124mAVG_PR_CONDITION: avg_pr_condition\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124mPERSISTENT_RESOURCE_ID: ai-takeoff\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124mREPLICA_COUNT: 1\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124mSERVICE_ACCOUNT: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSERVICE_ACCOUNT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ID' is not defined"
     ]
    }
   ],
   "source": [
    "config = f\"\"\"\n",
    "PROJECT_ID: {PROJECT_ID}\n",
    "BUCKET_NAME: {BUCKET_NAME}\n",
    "REGION: {REGION}\n",
    "ID: \"{ID}\"\n",
    "CUSTOMER_ENTITY_ID: customer\n",
    "CUSTOMER_ENTITY_ID_FIELD: customer_id\n",
    "TERMINAL_ENTITY_ID: terminal\n",
    "TERMINALS_ENTITY_ID_FIELD: terminal_id\n",
    "FEATURESTORE_ID: \"fraudfinder_{ID}\"\n",
    "FEATUREVIEW_ID: \"fraudfinder_view_{ID}\"\n",
    "NETWORK: fraud-finder-network\n",
    "SUBNET: https://www.googleapis.com/compute/v1/projects/fraud-finder-lab/regions/us-central1/subnetworks/us-central1\n",
    "MODEL_REGISTRY: ff_model\n",
    "RAW_BQ_TRANSACTION_TABLE_URI: \"{PROJECT_ID}.tx.tx\"\n",
    "RAW_BQ_LABELS_TABLE_URI: \"{PROJECT_ID}.tx.txlabels\"\n",
    "FEATURES_BQ_TABLE_URI: \"{PROJECT_ID}.tx.wide_features_table\"\n",
    "FEATURE_TIME: feature_ts\n",
    "ONLINE_STORAGE_NODES: 1\n",
    "SUBSCRIPTION_NAME: ff-tx-for-feat-eng-sub\n",
    "SUBSCRIPTION_PATH: \"projects/{PROJECT_ID}/subscriptions/ff-tx-for-feat-eng-sub\"\n",
    "DROP_COLUMNS:\n",
    "- timestamp\n",
    "- entity_type_customer\n",
    "- entity_type_terminal\n",
    "FEAT_COLUMNS:\n",
    "- customer_id_avg_amount_14day_window\n",
    "- customer_id_avg_amount_15min_window\n",
    "- customer_id_avg_amount_1day_window\n",
    "- customer_id_avg_amount_30min_window\n",
    "- customer_id_avg_amount_60min_window\n",
    "- customer_id_avg_amount_7day_window\n",
    "- customer_id_nb_tx_14day_window\n",
    "- customer_id_nb_tx_15min_window\n",
    "- customer_id_nb_tx_1day_window\n",
    "- customer_id_nb_tx_30min_window\n",
    "- customer_id_nb_tx_60min_window\n",
    "- customer_id_nb_tx_7day_window\n",
    "- terminal_id_avg_amount_15min_window\n",
    "- terminal_id_avg_amount_30min_window\n",
    "- terminal_id_avg_amount_60min_window\n",
    "- terminal_id_nb_tx_14day_window\n",
    "- terminal_id_nb_tx_15min_window\n",
    "- terminal_id_nb_tx_1day_window\n",
    "- terminal_id_nb_tx_30min_window\n",
    "- terminal_id_nb_tx_60min_window\n",
    "- terminal_id_nb_tx_7day_window\n",
    "- terminal_id_risk_14day_window\n",
    "- terminal_id_risk_1day_window\n",
    "- terminal_id_risk_7day_window\n",
    "- tx_amount\n",
    "TARGET_COLUMN: tx_fraud\n",
    "DATA_SCHEMA:\n",
    "  timestamp: object\n",
    "  tx_amount: float64\n",
    "  tx_fraud: Int64\n",
    "  entity_type_customer: Int64\n",
    "  customer_id_nb_tx_1day_window: Int64\n",
    "  customer_id_nb_tx_7day_window: Int64\n",
    "  customer_id_nb_tx_14day_window: Int64\n",
    "  customer_id_avg_amount_1day_window: float64\n",
    "  customer_id_avg_amount_7day_window: float64\n",
    "  customer_id_avg_amount_14day_window: float64\n",
    "  customer_id_nb_tx_15min_window: Int64\n",
    "  customer_id_avg_amount_15min_window: float64\n",
    "  customer_id_nb_tx_30min_window: Int64\n",
    "  customer_id_avg_amount_30min_window: float64\n",
    "  customer_id_nb_tx_60min_window: Int64\n",
    "  customer_id_avg_amount_60min_window: float64\n",
    "  entity_type_terminal: Int64\n",
    "  terminal_id_nb_tx_1day_window: Int64\n",
    "  terminal_id_nb_tx_7day_window: Int64\n",
    "  terminal_id_nb_tx_14day_window: Int64\n",
    "  terminal_id_risk_1day_window: float64\n",
    "  terminal_id_risk_7day_window: float64\n",
    "  terminal_id_risk_14day_window: float64\n",
    "  terminal_id_nb_tx_15min_window: Int64\n",
    "  terminal_id_avg_amount_15min_window: float64\n",
    "  terminal_id_nb_tx_30min_window: Int64\n",
    "  terminal_id_avg_amount_30min_window: float64\n",
    "  terminal_id_nb_tx_60min_window: Int64\n",
    "  terminal_id_avg_amount_60min_window: float64\n",
    "MODEL_NAME: ff_model\n",
    "EXPERIMENT_NAME: \"ff-experiment-{ID}\"\n",
    "DATA_URI: \"gs://{PROJECT_ID}-fraudfinder/data\"\n",
    "TRAIN_DATA_URI: \"gs://{PROJECT_ID}-fraudfinder/data/train\"\n",
    "READ_INSTANCES_TABLE: \"ground_truth_{ID}\"\n",
    "READ_INSTANCES_URI: \"bq://{PROJECT_ID}.tx.ground_truth_{ID}\"\n",
    "DATASET_NAME: \"fraud_finder_dataset_{ID}\"\n",
    "JOB_NAME: \"fraudfinder-train-xgb-{ID}\"\n",
    "ENDPOINT_NAME: ff_model_endpoint\n",
    "MODEL_SERVING_IMAGE_URI: \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\"\n",
    "IMAGE_REPOSITORY: \"fraudfinder-{ID}\"\n",
    "IMAGE_NAME: dask-xgb-classificator\n",
    "IMAGE_TAG: latest\n",
    "IMAGE_URI: \"us-central1-docker.pkg.dev/{PROJECT_ID}/fraudfinder-{ID}/dask-xgb-classificator:latest\"\n",
    "TRAIN_COMPUTE: e2-standard-4\n",
    "DEPLOY_COMPUTE: n1-standard-4\n",
    "BASE_IMAGE: \"python:3.10\"\n",
    "PIPELINE_NAME: \"fraud-finder-xgb-pipeline-{ID}\"\n",
    "PIPELINE_ROOT: \"gs://{PROJECT_ID}-fraudfinder/pipelines\"\n",
    "BQ_DATASET: tx\n",
    "METRICS_URI: \"gs://{PROJECT_ID}-fraudfinder/deliverables/metrics.json\"\n",
    "AVG_PR_THRESHOLD: 0.2\n",
    "MODEL_THRESHOLD: 0.5\n",
    "AVG_PR_CONDITION: avg_pr_condition\n",
    "PERSISTENT_RESOURCE_ID: ai-takeoff\n",
    "REPLICA_COUNT: 1\n",
    "SERVICE_ACCOUNT: \"{SERVICE_ACCOUNT}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the project id\n",
    "import requests\n",
    "\n",
    "\n",
    "# Detect Cloud project from environment\n",
    "headers = {\"Metadata-Flavor\": \"Google\"}\n",
    "PROJECT_ID = requests.get(\"http://metadata.google.internal/computeMetadata/v1/project/project-id\", headers=headers)\n",
    "PROJECT_ID = PROJECT_ID.content.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from utils import gcs_read, VertexConfig\n",
    "\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "config_path = \"config/vertex_conf.yaml\"\n",
    "\n",
    "with gcs_read(PROJECT_ID, BUCKET_NAME, \"config/vertex_conf.yaml\").open(\"r\") as f:\n",
    "    conf = yaml.safe_load(f)\n",
    "vertex_config = VertexConfig(**conf)\n",
    "#print(vertex_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud123-438914\n",
      "fraud123-438914\n",
      "us-central1\n"
     ]
    }
   ],
   "source": [
    "print(vertex_config.PROJECT_ID)\n",
    "print(PROJECT_ID)\n",
    "print(REGION)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-fraudfinder\"\n",
    "vertex_config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "print(vertex_config.n)\n",
    "exec(vertex_config.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "from typing import Union\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Model Training with Vertex AI\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Model Deployment and Evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import Featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "DATA_DIR = os.path.join(os.pardir, \"data\")\n",
    "TRAIN_DATA_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "\n",
    "# Feature Store\n",
    "START_DATE_TRAIN = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "SERVING_FEATURE_IDS = {vertex_config.CUSTOMER_ENTITY_ID: [\"*\"], vertex_config.TERMINAL_ENTITY_ID: [\"*\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Initialize Vertex AI SDK and BigQuery Client for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=vertex_config.PROJECT_ID, location=vertex_config.REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent Resource found in us-central1\n"
     ]
    }
   ],
   "source": [
    "# Persistent Resource ID\n",
    "PERSISTENT_RESOURCE_ID = \"ai-takeoff\"\n",
    "\n",
    "# Dynamically retrieve Persistent Resource location\n",
    "PERSISTENT_RESOURCE_REGION = \"\"\n",
    "check_regions = [\"us-central1\", \"asia-southeast1\", \"europe-west4\"]\n",
    "\n",
    "for region in check_regions:\n",
    "    shell_output = !gcloud ai persistent-resources list --project=$PROJECT_ID --region=$region\n",
    "    if \"Listed 0 items.\" not in shell_output:\n",
    "        print(f\"Persistent Resource found in {region}\")\n",
    "        PERSISTENT_RESOURCE_REGION = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://us-central1-aiplatform.googleapis.com/]', \"createTime: '2024-11-15T06:06:32.629787Z'\", 'displayName: ai-takeoff', 'error: {}', 'name: projects/520607199607/locations/us-central1/persistentResources/ai-takeoff', 'resourcePools:', '- diskSpec:', '    bootDiskSizeGb: 100', '    bootDiskType: pd-ssd', '  id: n1-standard-8', '  machineSpec:', '    machineType: n1-standard-8', \"  replicaCount: '1'\", 'resourceRuntimeSpec:', '  serviceAccountSpec:', '    enableCustomServiceAccount: true', \"startTime: '2024-11-15T06:12:21.455250762Z'\", 'state: RUNNING', \"updateTime: '2024-11-15T06:18:36.758653Z'\"]\n",
      "MACHINE_TYPE: n1-standard-8\n",
      "REPLICA_COUNT: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "DESCRIBE_PR_OUTPUT = !gcloud ai persistent-resources describe $PERSISTENT_RESOURCE_ID --project=$vertex_config.PROJECT_ID --region=$PERSISTENT_RESOURCE_REGION\n",
    "#print(DESCRIBE_PR_OUTPUT) \n",
    "\n",
    "# Join the output lines with spaces\n",
    "PR_DETAILS = \" \".join(DESCRIBE_PR_OUTPUT)  \n",
    "\n",
    "# Extract machine type\n",
    "match = re.search(r\"machineType: (\\w+-\\w+-\\d+)\", PR_DETAILS)\n",
    "if match:\n",
    "    MACHINE_TYPE = match.group(1)\n",
    "    print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")\n",
    "else:\n",
    "    print(\"MACHINE_TYPE not found in output.\")\n",
    "\n",
    "# Extract replica count\n",
    "match = re.search(r\"replicaCount: '(\\d+)'\", PR_DETAILS)\n",
    "if match:\n",
    "    REPLICA_COUNT = int(match.group(1))\n",
    "    print(f\"REPLICA_COUNT: {REPLICA_COUNT}\")\n",
    "else:\n",
    "    print(\"REPLICA_COUNT not found in output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7d80a641-2123-4954-b6b7-15a7f18a377a\" href=\"#view-view-vertex-resource-7d80a641-2123-4954-b6b7-15a7f18a377a\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7d80a641-2123-4954-b6b7-15a7f18a377a');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/ff-experiment-fvde2/runs?project=fraud123-438914');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/ff-experiment-fvde2/runs?project=fraud123-438914', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the default region for launching jobs.\n",
    "REGION = PERSISTENT_RESOURCE_REGION\n",
    "\n",
    "vertex_ai.init(\n",
    "    project=vertex_config.PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=vertex_config.BUCKET_NAME,\n",
    "    experiment=vertex_config.EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "You will now run some helper functions that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also using the BigQuery helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
    "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run a BigQuery query and return the job ID or result as a DataFrame\n",
    "    Args:\n",
    "        sql: SQL query, as a string, to execute in BigQuery\n",
    "    Returns:\n",
    "        df: DataFrame of results from query,  or error, if any\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Try dry run before executing query to catch any errors\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    # If dry run succeeds without errors, proceed to run query\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    client_result = bq_client.query(sql, job_config=job_config)\n",
    "\n",
    "    job_id = client_result.job_id\n",
    "\n",
    "    # Wait for query/job to finish running. then get & return data frame\n",
    "    df = client_result.result().to_arrow().to_pandas()\n",
    "    print(f\"Finished job_id: {job_id}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetching feature values for model training\n",
    "\n",
    "To fetch training data, we have to specify the following inputs to batch serving:\n",
    "\n",
    "- a file containing a \"query\", with the entities and timestamps for each label\n",
    "- a list of feature values to fetch\n",
    "- the destination location and format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-instance list\n",
    "\n",
    "In our case, we need a csv file with content formatted like the table below:\n",
    "\n",
    "|customer                     |terminal|timestamp                                    |\n",
    "|-----------------------------|--------|---------------------------------------------|\n",
    "|xxx3859                         |xxx8811    |2021-07-07 00:01:10 UTC                      |\n",
    "|xxx4165                         |xxx8810    |2021-07-07 00:01:55 UTC                      |\n",
    "|xxx2289                         |xxx2081    |2021-07-07 00:02:12 UTC                      |\n",
    "|xxx3227                         |xxx3011    |2021-07-07 00:03:23 UTC                      |\n",
    "|xxx2819                         |xxx6263    |2021-07-07 00:05:30 UTC                      |\n",
    "\n",
    "where the column names are the names of entities in Feature Store and the timestamps represents the time an event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE OR REPLACE TABLE `fraud123-438914.tx.ground_truth_fvde2` as (\n",
      "    SELECT\n",
      "        raw_tx.TX_TS AS timestamp,\n",
      "        raw_tx.CUSTOMER_ID AS customer,\n",
      "        raw_tx.TERMINAL_ID AS terminal,\n",
      "        raw_tx.TX_AMOUNT AS tx_amount,\n",
      "        raw_lb.TX_FRAUD AS tx_fraud,\n",
      "    FROM \n",
      "        tx.tx as raw_tx\n",
      "    LEFT JOIN \n",
      "        tx.txlabels as raw_lb\n",
      "    ON raw_tx.TX_ID = raw_lb.TX_ID\n",
      "    WHERE\n",
      "        DATE(raw_tx.TX_TS) = \"2024-11-15\"\n",
      "    LIMIT 50000\n",
      ");\n",
      "\n",
      "Finished job_id: 3e8e9ecc-ec37-4edf-ace6-ff7f23a348d7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{vertex_config.PROJECT_ID}.tx.{vertex_config.READ_INSTANCES_TABLE}` as (\n",
    "    SELECT\n",
    "        raw_tx.TX_TS AS timestamp,\n",
    "        raw_tx.CUSTOMER_ID AS customer,\n",
    "        raw_tx.TERMINAL_ID AS terminal,\n",
    "        raw_tx.TX_AMOUNT AS tx_amount,\n",
    "        raw_lb.TX_FRAUD AS tx_fraud,\n",
    "    FROM \n",
    "        tx.tx as raw_tx\n",
    "    LEFT JOIN \n",
    "        tx.txlabels as raw_lb\n",
    "    ON raw_tx.TX_ID = raw_lb.TX_ID\n",
    "    WHERE\n",
    "        DATE(raw_tx.TX_TS) = \"{START_DATE_TRAIN}\"\n",
    "    LIMIT 50000\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(sql_query)\n",
    "\n",
    "run_bq_query(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Feature Store ID\n",
    "Initiate the feature store you created in the `02_feature_engineering_batch.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ff_feature_store = Featurestore(vertex_config.FEATURESTORE_ID)\n",
    "except NameError:\n",
    "    print(f\"\"\"The feature store {vertex_config.FEATURESTORE_ID} does not exist!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch a sample of data and dump it into a bucket \n",
    "In this section, we will use the batch serving of the Vertex AI Feature Store to prepare a dataset for training.\n",
    "\n",
    "You first have to set `uniformbucketlevelaccess` on the bucket.  When you enable uniform bucket-level access on a bucket, Access Control Lists (ACLs) are disabled, and only bucket-level Identity and Access Management (IAM) permissions grant access to that bucket and the objects it contains. This is not the best practice for product workloads. We only use it to prevent issues when running the workshop. Read more about `uniformbucketlevelaccess` in our [documentation](https://cloud.google.com/storage/docs/uniform-bucket-level-access). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Uniform bucket-level access for gs://fraud123-438914-fraudfinder...\n"
     ]
    }
   ],
   "source": [
    "!gsutil uniformbucketlevelaccess set on gs://{vertex_config.BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next fetch a batch of data from the Vertex AI Feature Store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving Featurestore feature values: projects/520607199607/locations/us-central1/featurestores/fraudfinder_fvde2\n",
      "Serve Featurestore feature values backing LRO: projects/520607199607/locations/us-central1/featurestores/fraudfinder_fvde2/operations/1909122068402667520\n",
      "Featurestore feature values served. Resource name: projects/520607199607/locations/us-central1/featurestores/fraudfinder_fvde2\n",
      "Disabling Uniform bucket-level access for gs://fraud123-438914-fraudfinder...\n",
      "PreconditionException: 412 Request violates constraint 'constraints/storage.uniformBucketLevelAccess'\n"
     ]
    }
   ],
   "source": [
    "ff_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix=vertex_config.TRAIN_DATA_URI,\n",
    "    gcs_destination_type=\"csv\",\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=vertex_config.READ_INSTANCES_URI,\n",
    "    pass_through_fields=[\"tx_amount\", \"tx_fraud\"],\n",
    ")\n",
    "\n",
    "!gsutil uniformbucketlevelaccess set off gs://{vertex_config.BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will create a copy of the training data in your local notebook instance so that you can use it later for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://fraud123-438914-fraudfinder/data/train/000000000000.csv\n",
      "Copying gs://fraud123-438914-fraudfinder/data/train/000000000000.csv...\n",
      "/ [1 files][  8.2 MiB/  8.2 MiB]                                                \n",
      "Operation completed over 1 objects/8.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $vertex_config.TRAIN_DATA_URI\n",
    "!sudo gsutil cp -r $vertex_config.TRAIN_DATA_URI $TRAIN_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the features into cloud storage will generate a csv file. Let's list the local file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!ls $TRAIN_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a fraud detection model using Vertex AI custom training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Vertex AI dataset\n",
    "In this section, you will create a managed [Vertex AI dataset](https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets). Vertex AI datasets can be used to train AutoML models or custom-trained models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://fraud123-438914-fraudfinder/data/train/000000000000.csv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve list of local files\n",
    "flist = !ls $TRAIN_DATA_DIR\n",
    "obj_list = [f\"gs://{vertex_config.PROJECT_ID}-fraudfinder/data/train/{fname}\" for fname in flist]\n",
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/520607199607/locations/us-central1/datasets/5544411078118604800/operations/3443301428964098048\n",
      "TabularDataset created. Resource name: projects/520607199607/locations/us-central1/datasets/5544411078118604800\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/520607199607/locations/us-central1/datasets/5544411078118604800')\n",
      "Dataset: fraud_finder_dataset_fvde2\n",
      "Name: \t projects/520607199607/locations/us-central1/datasets/5544411078118604800\n"
     ]
    }
   ],
   "source": [
    "# create Vertex AI managed dataset\n",
    "dataset = vertex_ai.TabularDataset.create(\n",
    "    display_name=vertex_config.DATASET_NAME,\n",
    "    gcs_source=obj_list[0],\n",
    ")\n",
    "print(\"Dataset:\", f\"{dataset.display_name}\")\n",
    "print(\"Name: \\t\", f\"{dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom model\n",
    "\n",
    "In this section, you will need to train an XGBoost model on Vertex AI custom training. Custom training on Vertex AI requires a container, which contains all of the necessary code, files, and code dependencies needed to train the model.\n",
    "\n",
    "#### Create the training job with XGBoost and Dask\n",
    "\n",
    "To perform custom training, you can use either a pre-built container or build your container. In this notebook we will being use XGBoost with the Dask framework, and so we will need to build a custom container for XGBoost and use it to train a model with the Vertex AI custom training service.\n",
    "\n",
    "You will use Dask. Dask is a parallel computing library built on Python. Dask allows easy management of distributed workers and excels at handling large distributed data science workflows. The implementation in XGBoost originates from dask-xgboost with some extended functionalities and a different interface. \n",
    "\n",
    "##### Vertex AI and containers\n",
    "The first step is to write your training code. Then, you will need to write a Dockerfile and build a container image based on it. The following cell writes our code into `train_xgb.py`, the module for training an XGBClassifier. We will copy this code into our container to run through the Vertex AI training service.\n",
    "\n",
    "A custom container is a Docker image that you create to run your training application. By running your machine learning (ML) training job in a custom container, you can use ML frameworks, non-ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI. You can read more in our [documentation](https://cloud.google.com/vertex-ai/docs/training/containers-overview). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a folder for all container-related files\n",
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"a\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_training/train_xgb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/train_xgb.py\n",
    "\n",
    "\"\"\"\n",
    "train_xgb.py is the module for training a XGBClassifier pipeline\n",
    "\"\"\"\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import dask.dataframe as dask_df\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (roc_curve, confusion_matrix, average_precision_score, f1_score, \n",
    "                            log_loss, precision_score, recall_score)\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Read environmental variables\n",
    "def gcs_path_to_local_path(old_path: str) -> str:\n",
    "    new_path = old_path.replace(\"gs://\", \"/gcs/\")\n",
    "    return new_path\n",
    "\n",
    "TRAINING_DATA_PATH = gcs_path_to_local_path(os.environ[\"AIP_TRAINING_DATA_URI\"])\n",
    "TEST_DATA_PATH = gcs_path_to_local_path(os.environ[\"AIP_TEST_DATA_URI\"])\n",
    "MODEL_DIR = gcs_path_to_local_path(os.environ[\"AIP_MODEL_DIR\"])\n",
    "MODEL_PATH = MODEL_DIR + \"model.bst\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writetemplate build_training/train_xgb.py\n",
    "\n",
    "TARGET_COLUMN = \"{vertex_config.TARGET_COLUMN}\"\n",
    "FEAT_COLUMNS = {vertex_config.FEAT_COLUMNS}\n",
    "DROP_COLUMNS = {vertex_config.DROP_COLUMNS}\n",
    "DATA_SCHEMA = {vertex_config.DATA_SCHEMA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to build_training/train_xgb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a build_training/train_xgb.py\n",
    "\n",
    "## Training variables\n",
    "# Helpers -----------------------------------------------------------------------------------------------------------------------------\n",
    "def get_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--bucket\", dest=\"bucket\", type=str,\n",
    "                        required=True, help=\"Bucket uri\")\n",
    "    parser.add_argument(\"--max_depth\", dest=\"max_depth\",\n",
    "                        default=6, type=int,\n",
    "                        help=\"max_depth value.\")\n",
    "    parser.add_argument(\"--eta\", dest=\"eta\",\n",
    "                        default=0.4, type=float,\n",
    "                        help=\"eta.\")\n",
    "    parser.add_argument(\"--gamma\", dest=\"gamma\",\n",
    "                        default=0.0, type=float,\n",
    "                        help=\"eta value\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def resample(df: dask_df.DataFrame, replace: bool, frac: float = 1, random_state: int = 8) -> dask_df.DataFrame:\n",
    "    shuffled_df = df.sample(frac=frac, replace=replace, random_state=random_state)\n",
    "    return shuffled_df\n",
    "\n",
    "def preprocess(df: dask_df.DataFrame, drop_cols: List[str] = None) -> dask_df.DataFrame:\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Drop rows with NaN\"s\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"float32\", \"float64\"]).columns\n",
    "    numeric_format = {col:\"float32\" for col in numeric_columns}\n",
    "    df = df.astype(numeric_format)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model: xgb.Booster, x_true: Union[dask_df.DataFrame, np.ndarray], y_true: Union[dask_df.Series, np.ndarray]) -> dict:\n",
    "    y_true = y_true.compute()\n",
    "    \n",
    "    #calculate metrics\n",
    "    metrics={}\n",
    "    \n",
    "    y_score =  model.predict_proba(x_true)[:, 1]\n",
    "    y_score = y_score.compute()\n",
    "    fpr, tpr, thr = roc_curve(\n",
    "         y_true=y_true, y_score=y_score, pos_label=True\n",
    "    )\n",
    "    fpr_list = fpr.tolist()[::1000]\n",
    "    tpr_list = tpr.tolist()[::1000]\n",
    "    thr_list = thr.tolist()[::1000]\n",
    "\n",
    "    y_pred = model.predict(x_true)\n",
    "    y_pred = y_pred.compute()\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    avg_precision_score = round(average_precision_score(y_true, y_score), 3)\n",
    "    f1 = round(f1_score(y_true, y_pred), 3)\n",
    "    lg_loss = round(log_loss(y_true, y_pred), 3)\n",
    "    prec_score = round(precision_score(y_true, y_pred), 3)\n",
    "    rec_score = round(recall_score(y_true, y_pred), 3)\n",
    "    \n",
    "    metrics[\"fpr\"] = [round(f, 3) for f in fpr_list]\n",
    "    metrics[\"tpr\"] = [round(f, 3) for f in tpr_list]\n",
    "    metrics[\"thrs\"] = [round(f, 3) for f in thr_list]\n",
    "    metrics[\"confusion_matrix\"] = c_matrix.tolist()\n",
    "    metrics[\"avg_precision_score\"] = avg_precision_score\n",
    "    metrics[\"f1_score\"] = f1\n",
    "    metrics[\"log_loss\"] = lg_loss\n",
    "    metrics[\"precision_score\"] = prec_score\n",
    "    metrics[\"recall_score\"] = rec_score\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "        \n",
    "    # variables\n",
    "    bucket = gcs_path_to_local_path(args.bucket)\n",
    "    deliverable_uri = (Path(bucket)/\"deliverables\")\n",
    "    metrics_uri = (deliverable_uri/\"metrics.json\")\n",
    "\n",
    "    # read data\n",
    "    train_df = dask_df.read_csv(TRAINING_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    test_df = dask_df.read_csv(TEST_DATA_PATH, dtype=DATA_SCHEMA)\n",
    "    \n",
    "    # preprocessing\n",
    "    preprocessed_train_df = preprocess(train_df, DROP_COLUMNS)\n",
    "    preprocessed_test_df = preprocess(test_df, DROP_COLUMNS)\n",
    "    \n",
    "    # downsampling\n",
    "    train_nfraud_df = preprocessed_train_df[preprocessed_train_df[TARGET_COLUMN]==0]\n",
    "    train_fraud_df = preprocessed_train_df[preprocessed_train_df[TARGET_COLUMN]==1]\n",
    "    train_nfraud_downsample = resample(train_nfraud_df,\n",
    "                          replace=True, \n",
    "                          frac=len(train_fraud_df)/len(train_df))\n",
    "    ds_preprocessed_train_df = dask_df.concat([train_nfraud_downsample, train_fraud_df])\n",
    "    \n",
    "    # target, features split\n",
    "    x_train = ds_preprocessed_train_df[FEAT_COLUMNS].values\n",
    "    y_train = ds_preprocessed_train_df.loc[:, TARGET_COLUMN].astype(int).values\n",
    "    x_true = preprocessed_test_df[FEAT_COLUMNS].values\n",
    "    y_true = preprocessed_test_df.loc[:, TARGET_COLUMN].astype(int).values\n",
    "    \n",
    "    # train model\n",
    "    cluster =  LocalCluster()\n",
    "    client = Client(cluster)\n",
    "    model = xgb.dask.DaskXGBClassifier(objective=\"reg:logistic\", eval_metric=\"logloss\")\n",
    "    model.client = client\n",
    "    model.fit(x_train, y_train, eval_set=[(x_true, y_true)])\n",
    "    if not Path(MODEL_DIR).exists():\n",
    "        Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    model.save_model(MODEL_PATH)\n",
    "    \n",
    "    #generate metrics\n",
    "    metrics = evaluate_model(model, x_true, y_true)\n",
    "    if not Path(deliverable_uri).exists():\n",
    "        Path(deliverable_uri).mkdir(parents=True, exist_ok=True)\n",
    "    with open(metrics_uri, \"w\") as file:\n",
    "        json.dump(metrics, file, sort_keys = True, indent = 4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom image for Dask model training\n",
    "\n",
    "Now you will build a custom container. By running your training job in a custom container, you can use any ML framework, non-ML dependencies, libraries, and binaries. Next you will package your training code into a Docker container image, push the container image to Artifact Registry, and create a custom job on Vertex AI, which will use the container image on Artifact Registry. As the evolution of Container Registry, Artifact Registry is a single place for your organization to manage container images and language packages. It's fullly intergrated with the Vertex AI platform. You can read more in our [documentation](https://cloud.google.com/artifact-registry). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
      "Listing items under project fraud123-438914, across all locations.\n",
      "\n",
      "                                                                                ARTIFACT_REGISTRY\n",
      "REPOSITORY         FORMAT  MODE                 DESCRIPTION                          LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "fraudfinder-8wc8m  DOCKER  STANDARD_REPOSITORY  FraudFinder Docker Image repository  us-central1          Google-managed key  2024-11-02T15:07:21  2024-11-02T15:07:21  0\n",
      "fraudfinder-fvde2  DOCKER  STANDARD_REPOSITORY  FraudFinder Docker Image repository  us-central1          Google-managed key  2024-10-20T15:28:49  2024-11-14T16:29:05  2154.540\n",
      "Encryption: Google-managed key\n",
      "Registry URL: us-central1-docker.pkg.dev/fraud123-438914/fraudfinder-fvde2\n",
      "Repository Size: 2259.199MB\n",
      "createTime: '2024-10-20T15:28:49.803003Z'\n",
      "description: FraudFinder Docker Image repository\n",
      "format: DOCKER\n",
      "mode: STANDARD_REPOSITORY\n",
      "name: projects/fraud123-438914/locations/us-central1/repositories/fraudfinder-fvde2\n",
      "updateTime: '2024-11-14T16:29:05.673041Z'\n",
      "vulnerabilityScanningConfig:\n",
      "  enablementState: SCANNING_DISABLED\n",
      "  lastEnableTime: '2024-10-20T15:28:41.333552116Z'\n"
     ]
    }
   ],
   "source": [
    "# Create image repository\n",
    "!gcloud artifacts repositories create $vertex_config.IMAGE_REPOSITORY      --repository-format=docker      --location=$vertex_config.REGION     --description=\"FraudFinder Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list\n",
    "\n",
    "# Get info on the repository\n",
    "!gcloud artifacts repositories describe $vertex_config.IMAGE_REPOSITORY --location=$vertex_config.REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follow cell to allow this notebook to push to Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker $vertex_config.REGION-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to write your Dockerfile in order to create your container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "FROM python:3.10\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install gcsfs pyyaml numpy pandas scikit-learn dask distributed xgboost requests pydantic --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_xgb.py /root/train_xgb.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_xgb.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, build and push the Docker container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  11.26kB\n",
      "Step 1/5 : FROM python:3.10\n",
      " ---> 9ec1cdbafe6e\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 0991794de2de\n",
      "Step 3/5 : RUN pip install gcsfs pyyaml numpy pandas scikit-learn dask distributed xgboost requests pydantic --upgrade\n",
      " ---> Using cache\n",
      " ---> 60558020ba2f\n",
      "Step 4/5 : COPY ./train_xgb.py /root/train_xgb.py\n",
      " ---> Using cache\n",
      " ---> d7954103afb9\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"train_xgb.py\"]\n",
      " ---> Using cache\n",
      " ---> cbbb1f2e020b\n",
      "Successfully built cbbb1f2e020b\n",
      "Successfully tagged us-central1-docker.pkg.dev/fraud123-438914/fraudfinder-fvde2/dask-xgb-classificator:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/fraud123-438914/fraudfinder-fvde2/dask-xgb-classificator]\n",
      "\n",
      "\u001b[1Bc29422b9: Preparing \n",
      "\u001b[1Be79ef665: Preparing \n",
      "\u001b[1Bc3d676be: Preparing \n",
      "\u001b[1Bc003805e: Preparing \n",
      "\u001b[1B24527cde: Preparing \n",
      "\u001b[1B5e6144a7: Preparing \n",
      "\u001b[1B1bd83fe3: Preparing \n",
      "\u001b[1B071b5e0c: Preparing \n",
      "\u001b[8Be79ef665: Pushed    1.38GB/1.367GB\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2Klatest: digest: sha256:8a81c159fe8ce852c6e4850de533e6386cac32e2f5dad6e2e56671971948652f size: 2217\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Build and push Docker container\n",
    "!docker build -t $vertex_config.IMAGE_URI ./build_training/\n",
    "!docker push $vertex_config.IMAGE_URI\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a custom training job on Vertex AI\n",
    "Now that you have created your custom container, you will create a training job on Vertex AI. This will create a custom training job, load our dataset and register the model to Vertex AI Model Registry  after the training job is successfully completed. Learn more about the creaton of custom jobs [here](https://cloud.google.com/vertex-ai/docs/training/create-custom-job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account is ENABLED on Persistent Resource\n"
     ]
    }
   ],
   "source": [
    "# Check if Service Account is enabled on Persistent Resource\n",
    "SA_ENABLED = \"\"\n",
    "\n",
    "DESCRIBE_PR_OUTPUT = !gcloud ai persistent-resources describe $PERSISTENT_RESOURCE_ID --project=$PROJECT_ID --region=$PERSISTENT_RESOURCE_REGION\n",
    "PR_DETAILS = \" \".join(DESCRIBE_PR_OUTPUT)\n",
    "\n",
    "if \"enableCustomServiceAccount: true\" in PR_DETAILS:\n",
    "    SA_ENABLED = True\n",
    "    print(f\"Service Account is ENABLED on Persistent Resource\")\n",
    "else:\n",
    "    SA_ENABLED = False\n",
    "    print(f\"Service Account is NOT ENABLED on Persistent Resource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please proceed to next step to start the training job!\n"
     ]
    }
   ],
   "source": [
    "if SA_ENABLED:\n",
    "    print(\"Please proceed to next step to start the training job!\")\n",
    "else:\n",
    "    print(\"Please open the Terminal in a new tab and run the command -> gcloud auth login```\")\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Once done, continue on to start the training job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://fraud123-438914-fraudfinder/aiplatform-custom-training-2024-11-16-11:50:03.863 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1736387073692139520?project=520607199607\n",
      "CustomContainerTrainingJob projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/893475071559991296?project=520607199607\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/520607199607/locations/us-central1/trainingPipelines/1736387073692139520\n",
      "Model available at projects/520607199607/locations/us-central1/models/72731594665754624\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = f\"{vertex_config.MODEL_NAME}_xgb_{vertex_config.ID}\"\n",
    "\n",
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=vertex_config.JOB_NAME,\n",
    "    container_uri=vertex_config.IMAGE_URI,\n",
    "    model_serving_container_image_uri=vertex_config.MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "parameters = {\"MAX_DEPTH\": 4, \"ETA\": 0.3, \"GAMMA\": 0.1}\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--bucket={vertex_config.BUCKET_NAME}\",\n",
    "    \"--max_depth=\" + str(parameters[\"MAX_DEPTH\"]),\n",
    "    \"--eta=\" + str(parameters[\"ETA\"]),\n",
    "    \"--gamma=\" + str(parameters[\"GAMMA\"]),\n",
    "]\n",
    "\n",
    "\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_count=0,\n",
    "    persistent_resource_id=PERSISTENT_RESOURCE_ID,\n",
    "    service_account=vertex_config.SERVICE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model is training, you can visit the model URL, or go to the console page for [Vertex AI training jobs](https://console.cloud.google.com/vertex-ai/training/training-pipelines) to track its progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluate the model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can run the model via an endpoint, you need to transform the data so that the model can perform a prediction on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://fraud123-438914-fraudfinder/aiplatform-custom-training-2024-11-16-11:50:03.863/model/model.bst...\n",
      "/ [1 files][108.6 KiB/108.6 KiB]                                                \n",
      "Operation completed over 1 objects/108.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r $model.uri ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the model and get feature names\n",
    "bst = xgb.Booster()\n",
    "bst.load_model(\"./model/model.bst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=vertex_config.DROP_COLUMNS)\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    dummy_columns = list(df.dtypes[df.dtypes == \"category\"].index)\n",
    "    df = pd.get_dummies(df, columns=dummy_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# test set\n",
    "train_sample_path = os.path.join(TRAIN_DATA_DIR, \"000000000000.csv\")\n",
    "df_test = pd.read_csv(train_sample_path)\n",
    "preprocessed_test_Data = preprocess(df_test)\n",
    "\n",
    "x_test = preprocessed_test_Data[vertex_config.FEAT_COLUMNS].values\n",
    "y_test = preprocessed_test_Data.loc[:, vertex_config.TARGET_COLUMN].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will copy the model artifact to the local directory to evaluate the model localy before deploying the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9921992757754003, 0.9857320099255583, 0.9877827352314456, None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgtest = xgb.DMatrix(x_test)\n",
    "y_pred_prob = bst.predict(xgtest)\n",
    "y_pred = y_pred_prob.round().astype(int)\n",
    "y_pred_prob[0:10]\n",
    "precision_recall_fscore_support(y_test.values, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model\n",
    "Before you use your model to make predictions, you need to deploy it to an Endpoint. You can do this by calling the deploy function on the Model resource. This will do two things:\n",
    "\n",
    "- create an Endpoint resource\n",
    "- deploy the Model resource to the Endpoint resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/520607199607/locations/us-central1/endpoints/2600671554643689472/operations/6999245178493992960\n",
      "Endpoint created. Resource name: projects/520607199607/locations/us-central1/endpoints/2600671554643689472\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/520607199607/locations/us-central1/endpoints/2600671554643689472')\n",
      "Deploying model to Endpoint : projects/520607199607/locations/us-central1/endpoints/2600671554643689472\n",
      "Deploy Endpoint model backing LRO: projects/520607199607/locations/us-central1/endpoints/2600671554643689472/operations/5488850453464612864\n",
      "Endpoint model deployed. Resource name: projects/520607199607/locations/us-central1/endpoints/2600671554643689472\n"
     ]
    }
   ],
   "source": [
    "# Percentage of traffic that the model will receive in the endpoint\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "# Parameters to configure the minimum and maximum nodes during autoscaling\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=MODEL_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=vertex_config.DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the deployed model (Make an online prediction request)\n",
    "Send an online prediction request to your deployed model. To make sure your deployed model is working, test it out by sending a request to the endpoint.\n",
    "\n",
    "Let's first get a test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"instances\": x_test[:2].tolist()\n",
    "}\n",
    "\n",
    "# In case you want to test it in the console\n",
    "import json\n",
    "\n",
    "with open(\"predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0005512218340300024, 0.0003117244632449001], deployed_model_id='1615249651412238336', metadata=None, model_version_id='1', model_resource_name='projects/520607199607/locations/us-central1/models/72731594665754624', explanations=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(payload['instances'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Endpoint ID for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint ID: 2600671554643689472\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_endpoint_id(endpoint_name_substring, region=vertex_config.REGION):\n",
    "\n",
    "  try:\n",
    "    # Run the gcloud command and capture the output\n",
    "    command = f\"gcloud ai endpoints list --region {vertex_config.REGION}\"\n",
    "    process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = output.splitlines()\n",
    "\n",
    "    # Iterate through the lines and search for the endpoint name\n",
    "    for line in lines:\n",
    "      if endpoint_name_substring in line:\n",
    "        # Extract the endpoint ID from the line (assuming it's the first word)\n",
    "        endpoint_id = line.split()[0]\n",
    "        return endpoint_id\n",
    "\n",
    "    # If no matching endpoint is found\n",
    "    return None\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "#endpoint_name_substring =   # Replace with your substring\n",
    "endpoint_id = get_endpoint_id(MODEL_NAME)\n",
    "\n",
    "if endpoint_id:\n",
    "  print(f\"Endpoint ID: {endpoint_id}\")\n",
    "else:\n",
    "  print(\"Endpoint not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "endpoint_new = vertex_ai.Endpoint(f\"projects/{vertex_config.PROJECT_ID}/locations/{vertex_config.REGION}/endpoints/{endpoint_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0005512218340300024, 0.0003117244632449001], deployed_model_id='1615249651412238336', metadata=None, model_version_id='1', model_resource_name='projects/520607199607/locations/us-central1/models/72731594665754624', explanations=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_new.predict(payload['instances'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we understand we packaged our XGBoost model and started a custom training job on Vertex AI we can take the ML workflow and formalize it into a Vertex AI Pipeline.\n",
    "\n",
    "You can continue with the next Notebook: `06_formalization.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything under this is just sketch pad. To be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2024-11-15T06:06:32.629787Z'\n",
      "displayName: ai-takeoff\n",
      "error: {}\n",
      "name: projects/520607199607/locations/us-central1/persistentResources/ai-takeoff\n",
      "resourcePools:\n",
      "- diskSpec:\n",
      "    bootDiskSizeGb: 100\n",
      "    bootDiskType: pd-ssd\n",
      "  id: n1-standard-8\n",
      "  machineSpec:\n",
      "    machineType: n1-standard-8\n",
      "  replicaCount: '1'\n",
      "resourceRuntimeSpec:\n",
      "  serviceAccountSpec:\n",
      "    enableCustomServiceAccount: true\n",
      "startTime: '2024-11-15T06:12:21.455250762Z'\n",
      "state: RUNNING\n",
      "updateTime: '2024-11-15T06:18:36.758653Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai persistent-resources describe $PERSISTENT_RESOURCE_ID --project=$PROJECT_ID --region=$PERSISTENT_RESOURCE_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://us-central1-aiplatform.googleapis.com/]', '2600671554643689472  ff_model_xgb_fvde2_endpoint', '1508583829378433024  ff_model_xgb_frmlz_fvde2_endpoint']\n"
     ]
    }
   ],
   "source": [
    "endpoint_id = !gcloud ai endpoints list --region us-central1 | grep $MODEL_NAME\n",
    "print(endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint ID: 2600671554643689472\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_endpoint_id(endpoint_name_substring, region=vertex_config.REGION):\n",
    "\n",
    "  try:\n",
    "    # Run the gcloud command and capture the output\n",
    "    command = f\"gcloud ai endpoints list --region {vertex_config.REGION}\"\n",
    "    process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    output = process.stdout\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = output.splitlines()\n",
    "\n",
    "    # Iterate through the lines and search for the endpoint name\n",
    "    for line in lines:\n",
    "      if endpoint_name_substring in line:\n",
    "        # Extract the endpoint ID from the line (assuming it's the first word)\n",
    "        endpoint_id = line.split()[0]\n",
    "        return endpoint_id\n",
    "\n",
    "    # If no matching endpoint is found\n",
    "    return None\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "#endpoint_name_substring =   # Replace with your substring\n",
    "endpoint_id = get_endpoint_id(MODEL_NAME)\n",
    "\n",
    "if endpoint_id:\n",
    "  print(f\"Endpoint ID: {endpoint_id}\")\n",
    "else:\n",
    "  print(\"Endpoint not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
